{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatHuggingFace\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load an open-source LLM\n",
    "llm = HuggingFaceEndpoint(repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", task = \"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407 (Request ID: Root=1-67e2670c-00225eee3b69996b63f6a109;1b9f9e74-f8f1-493f-933f-f38abf90d930)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Capital of USA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    306\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    308\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    309\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    310\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    311\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    312\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    313\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    314\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    316\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    317\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:843\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    837\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    842\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:683\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    682\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 683\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    684\u001b[0m                 m,\n\u001b[0;32m    685\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    686\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    687\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    688\u001b[0m             )\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:908\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 908\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    909\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    910\u001b[0m         )\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_community\\chat_models\\huggingface.py:137\u001b[0m, in \u001b[0;36mChatHuggingFace._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m    136\u001b[0m llm_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_chat_prompt(messages)\n\u001b[1;32m--> 137\u001b[0m llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    138\u001b[0m     prompts\u001b[38;5;241m=\u001b[39m[llm_input], stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_chat_result(llm_result)\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1526\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1525\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1526\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1527\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m     )\n\u001b[0;32m   1530\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:267\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[0;32m    131\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:272\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m url \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mbuild_url(provider_helper\u001b[38;5;241m.\u001b[39mmap_model(model))\n\u001b[0;32m    271\u001b[0m headers \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_headers(headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:327\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407 (Request ID: Root=1-67e2670c-00225eee3b69996b63f6a109;1b9f9e74-f8f1-493f-933f-f38abf90d930)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "model.invoke(\"What is Capital of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of the United States of America (USA) is Washington, D.C. It was founded on July 16, 1790. The \"D.C.\" stands for District of Columbia, which was named after Christopher Columbus. Washington, D.C. is not part of any state and is instead a federal district under the exclusive jurisdiction of the U.S. Congress.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is Capital of USA\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model=ChatGoogleGenerativeAI(model='gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nThe capital of the United States of America (USA) is **Washington, D.C.** (District of Columbia).'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model.invoke(\"what is capital of USA\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "openai_model=ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai_model.invoke(\"what is capital of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"you are a helpful {domain} expert\"),\n",
    "        (\"human\",\"explain the given {topic} in simple terms\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({\"domain\":\"medical\",\"topic\":\"maleria\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='you are a helpful medical expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the given maleria in simple terms', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[SystemMessage(content='you are a helpful medical expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the maleria in simple terms', additional_kwargs={}, response_metadata={})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model=ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> openai_model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, I'd be happy to explain malaria in simple terms:\\n\\nMalaria is a serious and sometimes fatal disease caused by a parasite that commonly infects a certain type of mosquito which feeds on humans. Here's a simple breakdown:\\n\\n1. **The Cause**: A type of parasite called Plasmodium.\\n2. **The Carrier**: Mosquitoes, specifically those of the Anopheles genus.\\n3. **The Process**: When an infected mosquito bites a human, it injects the parasite into the bloodstream. The parasite then travels to the liver, where it matures and begins to infect red blood cells.\\n4. **The Symptoms**: These usually appear 10-15 days after being bitten, and can include:\\n   - Fever and chills\\n   - Nausea and vomiting\\n   - Fatigue and weakness\\n   - Sweats and body aches\\n   - Sometimes, symptoms can include diarrhea, headache, and a cough.\\n5. **The Treatment**: If diagnosed early, malaria can usually be treated with prescription medications. However, if not treated, it can lead to severe illness and even death.\", additional_kwargs={}, response_metadata={}, id='run-5a80e91c-d0f0-4a04-9d98-429340e92fe2-0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['domain', 'topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['domain'], input_types={}, partial_variables={}, template='you are a helpful {domain} expert'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='explain the given {topic} in simple terms'), additional_kwargs={})])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({\"domain\":\"education\",\"topic\":\"Data Science\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, I'd be happy to explain Data Science in simple terms!\\n\\nData Science is like being a detective who loves to find patterns and make predictions using data. Here's a simple breakdown:\\n\\n1. **Data**: It's the raw information that you have. It could be anything like customer purchases, weather information, or even your steps counted by your fitness tracker.\\n\\n2. **Science**: This is where you ask questions and try to find answers using the data. You're not just looking at the data, but you're trying to understand it, like a scientist.\\n\\n3. **Process**: Here's how a data scientist works:\\n\\n   - **Collect**: Gather data from different sources.\\n   - **Clean**: Make sure the data is in a usable format. This can involve fixing errors, removing irrelevant information, and more.\\n   - **Explore**: Look at the data to understand what it's telling you. This is where you start to see patterns and trends.\\n   - **Model**: Use the patterns you've found to create a model that can make predictions. For example, if you're studying weather data, your model might predict if it's going to rain tomorrow.\\n   - **Evaluate**: Check if your model is giving good predictions. If not, you might need to go back and adjust your model or collect more data.\\n   - **Communicate**: Share your findings with others. This could be in the form of a report, a presentation, or even a data visualization like a graph or chart.\\n\\nIn simple terms, Data Science is about finding insights and telling stories with data. It's used in many fields, from business to healthcare, to help make better decisions.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template=PromptTemplate(\n",
    "    template=\"can you say Welcome to {name} in 5 different ways\",\n",
    "    input_variables=['name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = template.invoke({\"name\":\"Yashswi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. \"Namaste, Yashswi! Welcome!\"\\n2. \"Hello Yashswi, nice to see you!\"\\n3. \"Hi Yashswi, welcome back!\"\\n4. \"Howdy Yashswi, great to have you here!\"\\n5. \"Salutations, Yashswi! Welcome to the conversation!\"'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using gemini model \n",
    "model.invoke(prompt_template).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAT PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"you are helpful {domain} assistant\"),\n",
    "        (\"human\",\"give me brief explanation about the {topic}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({\"domain\":\"education\",\"topic\":\"Data Mining\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, I'd be happy to explain data mining in a brief and simple way!\\n\\nData mining is like digging for hidden treasures in a mountain of data. Here's a simple breakdown:\\n\\n1. **Large Datasets**: Data mining deals with large and complex datasets. These could be from various sources like social media, customer transactions, or even sensors in the Internet of Things (IoT).\\n\\n2. **Extracting Patterns**: The goal of data mining is to extract useful patterns, trends, and correlations from these large datasets. These patterns can help in making informed decisions.\\n\\n3. **Techniques**: Data mining uses a variety of techniques, including:\\n   - **Clustering**: Grouping similar data points together.\\n   - **Classification**: Categorizing data into predefined groups.\\n   - **Association**: Finding relationships between items (like in market basket analysis).\\n   - **Regression**: Predicting a numerical value based on input data.\\n\\n4. **Applications**: Data mining is used in many fields, such as:\\n   - Business: To understand customer behavior and make better marketing decisions.\\n   - Healthcare: To analyze patient data for better diagnosis and treatment.\\n   - Science: To analyze experimental data and make discoveries.\\n\\nIn essence, data mining is about turning raw data into valuable insights and knowledge.\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gemini_model\n",
    "model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser() # StrOutputParser because we need output in string format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"can you give me a detailed explaination of {topic}\",\n",
    "    input_variables=['topic']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load an open-source LLM\n",
    "llm = HuggingFaceEndpoint(repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", task = \"text-generation\")\n",
    "huggingface_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Chaining** will automatically take care of invoking the prompt_template to the model, so, we don't need to do it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chaining --> Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple llm chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | huggingface_model | parser # parser is just returning the \"content\" part only, w/o the need of \n",
    "# writing \"model.invoke(prompt).content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Absolutely, I'd be happy to explain machine learning in detail!\\n\\nMachine Learning (ML) is a subset of artificial intelligence (AI) that involves training models on data to make predictions or decisions without being explicitly programmed. Here's a detailed breakdown:\\n\\n1. **Types of Machine Learning:**\\n\\n   - **Supervised Learning:** The model is trained on a labeled dataset, i.e., the input data has corresponding output values. The goal is to learn a mapping function from input to output. Examples include:\\n     - Linear Regression\\n     - Logistic Regression\\n     - Decision Trees\\n     - Random Forests\\n     - Support Vector Machines (SVM)\\n     - Naive Bayes\\n     - k-Nearest Neighbors (k-NN)\\n     - Neural Networks and Deep Learning models (like CNN, RNN, LSTM)\\n\\n   - **Unsupervised Learning:** The model is trained on an unlabeled dataset, i.e., the input data doesn't have corresponding output values. The goal is to find patterns, structure, or relationships within the data. Examples include:\\n     - K-Means Clustering\\n     - Hierarchical Clustering\\n     - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\\n     - Principal Component Analysis (PCA)\\n     - Association Rule Learning (Apriori, Eclat, FP-Growth)\\n     - Autoencoders (a type of neural network)\\n\\n   - **Semi-Supervised Learning:** This is a middle ground between supervised and unsupervised learning. It uses a small amount of labeled data and a large amount of unlabeled data for training.\\n\\n   - **Reinforcement Learning:** The model learns to make decisions by taking actions in an environment to maximize a reward signal. It's often used in robotics, gaming, and resource management. Examples include:\\n     - Q-Learning\\n     - State-Action-Reward-State-Action (SARSA)\\n     - Proximal Policy Optimization (PPO)\\n     - Deep Q-Network (DQN)\\n\\n2. **Machine Learning Workflow:**\\n\\n   - **Data Collection:** Gather data relevant to the problem you want to solve. Data can be numerical, categorical, textual, or even images and videos.\\n\\n   - **Data Preparation:** Clean and preprocess the data. This might involve handling missing values, outliers, normalizing features, and encoding categorical variables.\\n\\n   - **Exploratory Data Analysis (EDA):** Explore and understand the data's distribution\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\":\"Machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "# Visualization of Chain \n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating complex chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template = \"get a detailed report on {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"generate a 3 point summary from the following {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting the above two prompts using CHAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_chain = prompt1 | huggingface_model | parser | prompt2 | huggingface_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "# Visualization of Chain \n",
    "complex_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "result = complex_chain.invoke({\"topic\":\"machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**3-Point Summary:**\n",
      "\n",
      "1. **Definition**: Machine Learning (ML) is a subset of AI that enables computers to learn from data, recognize patterns, and make predictions or decisions without explicit programming.\n",
      "\n",
      "2. **Types**: ML algorithms can be categorized into three main types based on how they learn from data:\n",
      "   - Supervised Learning: Learns to map inputs to outputs using labeled data.\n",
      "   - Unsupervised Learning: Discovers patterns in unlabeled data.\n",
      "   - Reinforcement Learning: Learns to make decisions by taking actions in an environment and receiving rewards or penalties.\n",
      "\n",
      "3. **Key Algorithms**: Some of the most prominent ML algorithms include:\n",
      "   - Linear Regression & Logistic Regression for predictive modeling.\n",
      "   - Decision Trees & Random Forests for classification and regression.\n",
      "   - Support Vector Machines (SVM) for classification and regression.\n",
      "   - Naive Bayes for probabilistic classification.\n",
      "   - K-Means Clustering for unsupervised learning.\n",
      "   - K-Nearest Neighbors (KNN) for instance-based learning.\n",
      "   - Artificial Neural Networks (ANN) / Deep Learning (DL) for complex pattern recognition tasks.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template = \"Analyze the given text {text} carefully and take the necessary data\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"Summarize the given text {text} in bullet points\",\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"In November 2023, xAI began previewing Grok as a chatbot to selected people,[10] with participation in the early access program being limited to paid X Premium users.[11]\n",
    "\n",
    "It was announced that once the bot was out of early beta, it would only be available to higher tier X Premium+ subscribers.[12]\n",
    "\n",
    "At the time of the preview, xAI described the chatbot as \"a very early beta product – the best we could do with 2 months of training\" that could \"improve rapidly with each passing week\".[13]\n",
    "\n",
    "On March 11, 2024, Musk posted on X that the language model would go open source within a week. Six days later, on March 17, Grok-1 was open sourced under the Apache-2.0 license.[14][15] Disclosed were the networks architecture and its weight parameters.[16]\n",
    "\n",
    "On March 26, 2024, Musk announced that Grok would be enabled for premium subscribers, not just those on the higher-end tier, Premium+.[17]\n",
    "\n",
    "Grok-1.5\n",
    "Grok-1.5\n",
    "Developer(s)\txAI\n",
    "Initial release\tMay 15, 2024; 10 months ago\n",
    "Predecessor\tGrok-1.5\n",
    "Successor\tGrok-2\n",
    "Type\t\n",
    "Large language model\n",
    "Foundation model\n",
    "License\tProprietary\n",
    "Website\tx.ai/blog/grok-1.5\n",
    "On March 29, 2024, Grok-1.5 was announced, with \"improved reasoning capabilities\" and a context length of 128,000 tokens.[18] Grok-1.5 was released to all X Premium users on May 15, 2024.[1]\n",
    "\n",
    "On April 4, 2024, an update to X's \"Explore\" page included summaries of breaking news stories written by Grok, a task previously assigned to a human curation team.[19]\n",
    "\n",
    "On April 12, 2024, Grok-1.5 Vision (Grok-1.5V) was announced. Grok-1.5V is able to process a wide variety of visual information, including documents, diagrams, graphs, screenshots, and photographs.[20] Grok-1.5V was never released to the public.\n",
    "\n",
    "On May 4, 2024, Grok became available in the United Kingdom,[21] that being the only country in Europe to support Grok at the moment due to the impending Artificial Intelligence Act rules in the European Union. Grok was later reviewed by the EU and was released on May 16, 2024.[22]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_chain1 = prompt1 | huggingface_model | parser | prompt2 | huggingface_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\$\\Desktop\\GenAI_Bootcamp-main\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "result = complex_chain1.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summarized and structured analysis of the given text:\n",
      "\n",
      "**Timeline of xAI's Grok Chatbot Development and Release:**\n",
      "\n",
      "- **November 2023:**\n",
      "  - Preview of Grok as a chatbot to selected paid X Premium users.\n",
      "  - Post-beta availability announced for higher-tier X Premium+ subscribers.\n",
      "\n",
      "- **March 11, 2024:**\n",
      "  - Elon Musk announces open-sourcing of Grok within a week.\n",
      "\n",
      "- **March 17, 2024:**\n",
      "  - Grok-1 open sourced under Apache-2.0 license, revealing architecture and weight parameters.\n",
      "\n",
      "- **March 26, 2024:**\n",
      "  - Musk announces Grok for all premium subscribers, not just Premium+.\n",
      "\n",
      "- **March 29, 2024:**\n",
      "  - Announcement of Grok-1.5 with improved reasoning and 128,000 token context length.\n",
      "\n",
      "- **May 15, 2024:**\n",
      "  - Release of Grok-1.5 to all X Premium users.\n",
      "\n",
      "- **April 4, 2024:**\n",
      "  - Grok starts summarizing breaking news stories on X's \"Explore\" page.\n",
      "\n",
      "- **April 12, 2024:**\n",
      "  - Announcement of Grok-1.5 Vision (Grok-1.5V), capable of processing visual information, but never released.\n",
      "\n",
      "- **May 4, 2024:**\n",
      "  - Grok becomes available in the United Kingdom.\n",
      "\n",
      "- **May 16, 2024:**\n",
      "  - Grok released in the European Union after review.\n",
      "\n",
      "**Key Points:**\n",
      "- Grok's access expanded over time, starting as an early beta product.\n",
      "- It was open sourced initially, then became proprietary again for Grok-1.5.\n",
      "- Grok gained new capabilities like improved reasoning, visual processing, and news summarization.\n",
      "- Grok's release was subject to regional regulations, with the UK and EU being the first to support it.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = PromptTemplate(\n",
    "    template = \"Summarize the text {topic} given into 2 bullet points\",\n",
    "    input_variables = [\"topic\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = summary_prompt | huggingface_model | parser\n",
    "# result = summary_chain.invoke({\"topic\":\"Machine Learning\"})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template = \"generate simple notes from the following text \\n {text}\",\n",
    "    input_variables = [\"text\"]\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"generate 5 questions and answers from the following text \\n {text}\",\n",
    "    input_variables = [\"text\"]\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template = \"merge the provided notes and questions and answers into a single document \\n notes : {notes}, Q&A : {qa}\",\n",
    "    input_variables = [\"notes\",\"qa\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nThe capital of the United States is Washington, D.C.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 10, 'total_tokens': 27, 'completion_time': 0.061818182, 'prompt_time': 0.003575523, 'queue_time': 0.051837986, 'total_time': 0.065393705}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_fc872c073e', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f0c5d28-eb46-4f3e-90dc-bc97e7460314-0', usage_metadata={'input_tokens': 10, 'output_tokens': 17, 'total_tokens': 27})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model.invoke(\"What is the Capital of USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "    \"notes\" : prompt1 | groq_model | parser,\n",
    "    \"qa\" : prompt2 | groq_model | parser\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chain = prompt3 | groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = parallel_chain | merge_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            +-------------------------+            \n",
      "            | Parallel<notes,qa>Input |            \n",
      "            +-------------------------+            \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "           +--------------------------+            \n",
      "           | Parallel<notes,qa>Output |            \n",
      "           +--------------------------+            \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +----------------+                 \n",
      "                | PromptTemplate |                 \n",
      "                +----------------+                 \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                   +----------+                    \n",
      "                   | ChatGroq |                    \n",
      "                   +----------+                    \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +-----------------+                \n",
      "                | StrOutputParser |                \n",
      "                +-----------------+                \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "            +-----------------------+              \n",
      "            | StrOutputParserOutput |              \n",
      "            +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.[1]\n",
    "\n",
    "Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.[2]\n",
    "\n",
    "Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[3] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[4] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[5]\n",
    "\n",
    "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
    "\n",
    "Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[2]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "**Q1: What is an intelligent agent and what are its key characteristics?**\n",
      "\n",
      "An intelligent agent is a proactive entity that pursues goals, makes decisions, and takes actions over time, demonstrating a new form of digital agency. Its key characteristics include autonomy, goal-oriented behavior, and the ability to operate in complex environments, ranging from simple systems like a thermostat to complex entities like humans or firms.\n",
      "\n",
      "**Q2: How do intelligent agents operate in terms of their goals?**\n",
      "\n",
      "Intelligent agents operate by utilizing an objective function that defines their goals. They create and execute plans to maximize the expected value of this function. For example, reinforcement learning agents use a reward function, while evolutionary algorithms use a fitness function to guide their actions.\n",
      "\n",
      "**Q3: In which fields besides AI are intelligent agents studied?**\n",
      "\n",
      "Intelligent agents are studied in various fields beyond AI, including economics, cognitive science, ethics, philosophy of practical reason, socio-cognitive modeling, and computer social simulations.\n",
      "\n",
      "**Q4: What is an abstract intelligent agent?**\n",
      "\n",
      "An abstract intelligent agent is a theoretical model that represents the functional aspects of an intelligent agent, distinct from real-world implementations. It is closely related to software agents, which are autonomous programs designed to perform tasks for users.\n",
      "\n",
      "**Q5: Where does the term \"rational agent\" come from in the context of intelligent agents?**\n",
      "\n",
      "The term \"rational agent\" originates from economics, referring to agents that act autonomously to achieve their goals, much like economic agents making rational decisions.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parallel_chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notes': '<think>\\nOkay, so I need to help the user by generating simple notes from the given text about intelligent agents. Let me start by reading through the text carefully. \\n\\nHmm, the text begins by explaining that an intelligent agent, or AI agent, is proactive in pursuing goals, making decisions, and taking actions over time. It mentions that this represents a new form of digital agency. I should note that down as the first point.\\n\\nNext, the text says that intelligent agents can range from simple to complex. Examples given are a thermostat, a human being, a firm, a state, or a biome. I\\'ll include that as another point, listing the examples for clarity.\\n\\nThen it talks about the objective function, which defines the agent\\'s goals. The agents create and execute plans to maximize this function\\'s expected value. Examples provided are reinforcement learning with a reward function and evolutionary algorithms with a fitness function. I\\'ll break this into a separate point, explaining the function and its examples.\\n\\nThe text also relates intelligent agents in AI to those in economics and mentions that they\\'re studied in other fields like cognitive science, ethics, philosophy, and socio-cognitive modeling. I\\'ll make a note about the interdisciplinary aspects here.\\n\\nFinally, it describes intelligent agents as abstract functional systems similar to computer programs. It distinguishes between abstract models and real-world implementations, referring to them as abstract intelligent agents. It also connects them to software agents and the term \"rational agent\" from economics. I\\'ll summarize this as another point.\\n\\nI should make sure each point is clear and concise, using bullet points for easy reading. Let me organize these thoughts into simple notes without any markdown, keeping each point straightforward.\\n</think>\\n\\n**Simple Notes:**\\n\\n- **Intelligent Agent (AI Agent):**  \\n  - Proactively pursues goals, makes decisions, and takes actions over extended periods.  \\n  - Represents a new form of digital agency.\\n\\n- **Range of Complexity:**  \\n  - Can be simple (e.g., thermostat) to highly complex (e.g., human being, firm, state, or biome).\\n\\n- **Objective Function:**  \\n  - Defines the agent\\'s goals.  \\n  - Agents create and execute plans to maximize the expected value of this function.  \\n  - Examples:  \\n    - Reinforcement learning agent (reward function).  \\n    - Evolutionary algorithm (fitness function).\\n\\n- **Interdisciplinary Connections:**  \\n  - Related to agents in economics.  \\n  - Studied in cognitive science, ethics, philosophy of practical reason, and socio-cognitive modeling.\\n\\n- **Abstract Functional Systems:**  \\n  - Described as similar to computer programs.  \\n  - Distinguished as \"abstract intelligent agents\" in theoretical models.  \\n  - Related to software agents (autonomous programs) and the term \"rational agent\" (from economics).', 'qa': '<think>\\nOkay, so I need to generate five questions and answers based on the provided text about intelligent agents. Let me read through the text carefully to understand the key points.\\n\\nFirst, the text defines an intelligent agent as something that proactively pursues goals and takes actions over time. It mentions that agents can range from simple, like a thermostat, to complex, like a human or a firm. They operate based on an objective function, which includes goals, and they create plans to maximize this function. Examples given include reinforcement learning with a reward function and evolutionary algorithms with a fitness function.\\n\\nThe text also connects intelligent agents in AI to other fields like economics, cognitive science, and philosophy. It distinguishes between abstract intelligent agents and real-world implementations, and relates them to software agents and the term \"rational agent.\"\\n\\nNow, I need to create questions that cover these main points. Let\\'s see:\\n\\n1. The first paragraph introduces intelligent agents and their proactive nature. So a question about the definition and expansion of the AI agent concept would be good.\\n\\n2. The second paragraph gives examples of agents, from thermostats to humans. A question asking for examples of intelligent agents would fit here.\\n\\n3. The third paragraph discusses the objective function and how agents maximize it. A question about how agents operate and the role of the objective function makes sense.\\n\\n4. The text mentions related fields like economics and cognitive science, so a question about the interdisciplinary aspects would be appropriate.\\n\\n5. Finally, the last part distinguishes between abstract models and real-world agents, and relates them to software agents and the term \"rational agent.\" A question about the relationship between these terms would cover that.\\n\\nLet me make sure each question is clear and directly relates to the text. The answers should be concise, pulling information straight from the text without adding extra information.\\n\\nI think that covers all the main sections of the text. Now, I\\'ll structure each question and answer pair based on these points.\\n</think>\\n\\n**Question 1:**  \\nWhat is an intelligent agent, and how does it expand the concept of an AI agent?  \\n**Answer:**  \\nAn intelligent agent proactively pursues goals, makes decisions, and takes actions over extended periods, exemplifying a novel form of digital agency.\\n\\n**Question 2:**  \\nCan you provide examples of intelligent agents?  \\n**Answer:**  \\nExamples range from simple systems like thermostats to complex entities like human beings, firms, or states.\\n\\n**Question 3:**  \\nHow do intelligent agents operate, and what role does the objective function play?  \\n**Answer:**  \\nIntelligent agents operate by creating and executing plans to maximize their objective function, which encapsulates their goals, using examples like reward or fitness functions.\\n\\n**Question 4:**  \\nIn which interdisciplinary fields are intelligent agents studied?  \\n**Answer:**  \\nIntelligent agents are studied in economics, cognitive science, ethics, philosophy of practical reason, and socio-cognitive modeling.\\n\\n**Question 5:**  \\nHow are intelligent agents related to software agents and the term \"rational agent\"?  \\n**Answer:**  \\nIntelligent agents are closely related to software agents, which are autonomous programs, and are also referred to as \"rational agents,\" a term borrowed from economics.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand the parser now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parsing helps us to convert the data according to our requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template = \"generate a 3 point summary from given text \\n {text}\",\n",
    "    input_variables = [\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | groq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, I need to help the user by generating a 3-point summary from the given text. Let me start by reading the text carefully to understand the main ideas.\\n\\nThe text talks about intelligent agents, explaining that they go beyond typical AI by being proactive in pursuing goals and making decisions over time. So that\\'s the first point: defining intelligent agents and their proactive nature.\\n\\nNext, it mentions that these agents can vary from simple, like a thermostat, to complex systems like humans or organizations. They operate based on an objective function, using things like reward or fitness functions. That\\'s the second point about their range and operation.\\n\\nLastly, the text connects intelligent agents to other fields like economics and cognitive science, and distinguishes them from software agents. So the third point should cover their interdisciplinary aspects and relationship with other concepts.\\n\\nI think these three points capture the essence of the text. Now, I\\'ll structure them clearly for the summary.\\n</think>\\n\\n1. **Definition and Proactivity**: Intelligent agents are proactive entities that pursue goals, make decisions, and take actions over extended periods, exemplifying a new form of digital agency. They range from simple systems like thermostats to complex entities like humans or organizations.\\n\\n2. **Objective Function and Operation**: These agents operate based on an objective function that encapsulates their goals, creating and executing plans to maximize expected value. Examples include reinforcement learning agents guided by reward functions and evolutionary algorithms shaped by fitness functions.\\n\\n3. **Interdisciplinary Connections and Terminology**: Intelligent agents are studied across disciplines like economics, cognitive science, and philosophy. They are often described as abstract functional systems, closely related to software agents and sometimes referred to as \"rational agents\" in fields like economics.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 348, 'prompt_tokens': 311, 'total_tokens': 659, 'completion_time': 1.2654545449999999, 'prompt_time': 0.025137759, 'queue_time': 0.04854664, 'total_time': 1.290592304}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_fc872c073e', 'finish_reason': 'stop', 'logprobs': None}, id='run-1a90b962-d707-4dcc-bcd7-77c788be8374-0', usage_metadata={'input_tokens': 311, 'output_tokens': 348, 'total_tokens': 659})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | groq_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to create a three-point summary based on the provided text about intelligent agents. Let me read through the text carefully to understand the key points.\n",
      "\n",
      "The text starts by defining an intelligent agent as an AI system that proactively pursues goals, makes decisions, and takes actions over time, representing a new form of digital agency. It mentions that these agents can range from simple, like a thermostat, to complex, like a human being or even organizations. So, the first point should cover the definition and range of intelligent agents.\n",
      "\n",
      "Next, the text explains that these agents operate based on an objective function that encapsulates their goals. They create and execute plans to maximize the expected value of this function. It gives examples like reinforcement learning agents using a reward function and evolutionary algorithms using a fitness function. This seems important enough to be the second point, focusing on how they function and their goal-oriented nature.\n",
      "\n",
      "Lastly, the text connects intelligent agents to other fields like economics, cognitive science, and philosophy. It also distinguishes between abstract models and real-world implementations, mentioning terms like \"software agents\" and \"rational agents.\" This interdisciplinarity and the different terms used are the third key point.\n",
      "\n",
      "I should make sure each point is concise and captures the essence without getting too detailed. Now, I'll structure them into clear, numbered points.\n",
      "</think>\n",
      "\n",
      "1. **Definition and Range**: Intelligent agents, ranging from simple systems like thermostats to complex entities like humans or organizations, proactively pursue goals, make decisions, and act over time, exemplifying a new form of digital agency.\n",
      "\n",
      "2. **Functioning and Goals**: These agents operate using an objective function that defines their goals, creating and executing plans to maximize expected value. Examples include reinforcement learning agents using reward functions and evolutionary algorithms using fitness functions.\n",
      "\n",
      "3. **Interdisciplinary Connections**: Intelligent agents are studied across various fields, including economics, cognitive science, and philosophy. They are often termed as \"software agents\" or \"rational agents,\" distinguishing between abstract models and real-world implementations.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = groq_model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to generate a three-point summary from the given text about intelligent agents. Let me read through the text carefully to understand the main points.\n",
      "\n",
      "First, the text explains that intelligent agents are proactive, pursuing goals and making decisions over time, which is a new form of digital agency. That seems like a key point about their proactive nature and digital agency.\n",
      "\n",
      "Next, it mentions that these agents can range from simple, like a thermostat, to complex, including humans or even large systems like firms or states. This shows the spectrum of complexity and examples of agents, so that's another important point.\n",
      "\n",
      "Then, the text talks about how intelligent agents operate based on an objective function, which includes goals. They use functions like reward or fitness functions to guide their behavior, which is a crucial aspect of their design and operation.\n",
      "\n",
      "I should make sure each summary point captures these elements clearly and concisely. Maybe structure each point to cover the definition, range of complexity, and operational basis of intelligent agents.\n",
      "\n",
      "Wait, the user provided an example response. Let me check that. It has three points: proactive digital agency, ranging from simple to complex, and operating on objective functions. That aligns with what I thought. I should make sure my summary is similar but in my own words.\n",
      "\n",
      "I might need to paraphrase each point to avoid duplication. For example, instead of \"proactively pursuing goals,\" I could say \"actively striving towards objectives.\" Instead of \"reward or fitness functions,\" maybe \"using specific functions to guide behavior.\"\n",
      "\n",
      "Also, the text mentions relations to economics, cognitive science, etc. Should that be included? The example didn't, so maybe it's not necessary for a three-point summary unless it's a main point. The main focus seems to be on the nature and operation of intelligent agents.\n",
      "\n",
      "I think I have a good grasp now. Time to put it all together into three clear, concise points.\n",
      "</think>\n",
      "\n",
      "1. **Proactive Digital Agency**: Intelligent agents actively strive towards objectives, exemplifying a novel form of digital agency through goal pursuit and decision-making over extended periods.\n",
      "\n",
      "2. **Spectrum of Complexity**: These agents vary widely, from simple systems like thermostats to complex entities such as humans, firms, or states, all meeting similar criteria.\n",
      "\n",
      "3. **Operational Guidance**: Agents operate using objective functions, employing reward or fitness functions to guide their behavior and maximize expected outcomes, shaping their desired actions effectively.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to generate a three-point summary from the given text about intelligent agents. Let me read through the text carefully to understand the main points.\\n\\nFirst, the text explains that intelligent agents are proactive, pursuing goals and making decisions over time, which is a new form of digital agency. That seems like a key point about their proactive nature and digital agency.\\n\\nNext, it mentions that these agents can range from simple, like a thermostat, to complex, including humans or even large systems like firms or states. This shows the spectrum of complexity and examples of agents, so that\\'s another important point.\\n\\nThen, the text talks about how intelligent agents operate based on an objective function, which includes goals. They use functions like reward or fitness functions to guide their behavior, which is a crucial aspect of their design and operation.\\n\\nI should make sure each summary point captures these elements clearly and concisely. Maybe structure each point to cover the definition, range of complexity, and operational basis of intelligent agents.\\n\\nWait, the user provided an example response. Let me check that. It has three points: proactive digital agency, ranging from simple to complex, and operating on objective functions. That aligns with what I thought. I should make sure my summary is similar but in my own words.\\n\\nI might need to paraphrase each point to avoid duplication. For example, instead of \"proactively pursuing goals,\" I could say \"actively striving towards objectives.\" Instead of \"reward or fitness functions,\" maybe \"using specific functions to guide behavior.\"\\n\\nAlso, the text mentions relations to economics, cognitive science, etc. Should that be included? The example didn\\'t, so maybe it\\'s not necessary for a three-point summary unless it\\'s a main point. The main focus seems to be on the nature and operation of intelligent agents.\\n\\nI think I have a good grasp now. Time to put it all together into three clear, concise points.\\n</think>\\n\\n1. **Proactive Digital Agency**: Intelligent agents actively strive towards objectives, exemplifying a novel form of digital agency through goal pursuit and decision-making over extended periods.\\n\\n2. **Spectrum of Complexity**: These agents vary widely, from simple systems like thermostats to complex entities such as humans, firms, or states, all meeting similar criteria.\\n\\n3. **Operational Guidance**: Agents operate using objective functions, employing reward or fitness functions to guide their behavior and maximize expected outcomes, shaping their desired actions effectively.'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StrOutputParser().parse(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=PromptTemplate(\n",
    "    template=\"give me name, age and city from the provided text {text} \\n {format_instructions}\" ,\n",
    "    input_variables=['text'],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, My name is yashswi jain, my age is 21 and I belong to Karanja(Lad)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me name, age and city from the provided text Hello, My name is yashswi jain, my age is 21 and I belong to Karanja(Lad) \\n Return a JSON object.'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without JSON Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = groq_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nOkay, so the user provided a query where they want me to extract the name, age, and city from a given text and return it as a JSON object. Let me start by reading the text carefully.\\n\\nThe text says, \"Hello, My name is yashswi jain, my age is 21 and I belong to Karanja(Lad).\" Alright, I need to parse this. The name is clearly \"yashswi jain.\" I should make sure to get the spelling right, even if it\\'s unconventional.\\n\\nNext, the age is 21. That\\'s straightforward. Now, the city part says \"Karanja(Lad).\" I think the user is referring to Karanja Lad, which is a city in India. I should include both parts in the city field to be accurate.\\n\\nI need to structure this into a JSON object. The keys should be \"name,\" \"age,\" and \"city.\" The values will be the extracted information. I should ensure the JSON syntax is correct, with proper commas and quotation marks.\\n\\nI also need to make sure the JSON is well-formatted for readability, maybe with indentation. Let me double-check the text to confirm I haven\\'t missed anything. Yep, name, age, and city are all there. \\n\\nI should also consider if the user might have any other underlying needs. Maybe they\\'re working on data parsing or integration, so providing a clear and correct JSON is essential. They might use this for further processing or API calls, so accuracy is key.\\n\\nAlright, putting it all together, the JSON should have the name as a string, age as a number, and city as a string including both parts. That should fulfill the user\\'s request effectively.\\n</think>\\n\\nHere is the extracted information in JSON format:\\n\\n```json\\n{\\n  \"name\": \"yashswi jain\",\\n  \"age\": 21,\\n  \"city\": \"Karanja(Lad)\"\\n}\\n```' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 399, 'prompt_tokens': 46, 'total_tokens': 445, 'completion_time': 1.450909091, 'prompt_time': 0.00680723, 'queue_time': 0.05096085, 'total_time': 1.457716321}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_2834edf0f4', 'finish_reason': 'stop', 'logprobs': None} id='run-ea30ce12-fab9-4587-91d0-5ebd5e1cc023-0' usage_metadata={'input_tokens': 46, 'output_tokens': 399, 'total_tokens': 445}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user provided a query where they want me to extract the name, age, and city from a given text and return it as a JSON object. Let me start by reading the text carefully.\\n\\nThe text says, \"Hello, My name is yashswi jain, my age is 21 and I belong to Karanja(Lad).\" Alright, I need to parse this. The name is clearly \"yashswi jain.\" I should make sure to get the spelling right, even if it\\'s unconventional.\\n\\nNext, the age is 21. That\\'s straightforward. Now, the city part says \"Karanja(Lad).\" I think the user is referring to Karanja Lad, which is a city in India. I should include both parts in the city field to be accurate.\\n\\nI need to structure this into a JSON object. The keys should be \"name,\" \"age,\" and \"city.\" The values will be the extracted information. I should ensure the JSON syntax is correct, with proper commas and quotation marks.\\n\\nI also need to make sure the JSON is well-formatted for readability, maybe with indentation. Let me double-check the text to confirm I haven\\'t missed anything. Yep, name, age, and city are all there. \\n\\nI should also consider if the user might have any other underlying needs. Maybe they\\'re working on data parsing or integration, so providing a clear and correct JSON is essential. They might use this for further processing or API calls, so accuracy is key.\\n\\nAlright, putting it all together, the JSON should have the name as a string, age as a number, and city as a string including both parts. That should fulfill the user\\'s request effectively.\\n</think>\\n\\nHere is the extracted information in JSON format:\\n\\n```json\\n{\\n  \"name\": \"yashswi jain\",\\n  \"age\": 21,\\n  \"city\": \"Karanja(Lad)\"\\n}\\n```'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'yashswi jain', 'age': 21, 'city': 'Karanja(Lad)'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | groq_model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With JSON Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'yashswi jain', 'age': 21, 'city': 'Karanja(Lad)'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yashswi jain'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karanja(Lad)'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= template2 | groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"\"\"AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.[1]\n",
    "\n",
    "Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.[2]\n",
    "\n",
    "Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[3] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[4] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[5]\n",
    "\n",
    "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
    "\n",
    "Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[2]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['Intelligent agents can range from simple to highly complex, including examples such as a basic thermostat, a human being, or even large systems like firms or states.', 'These agents operate based on an objective function that encapsulates their goals, creating and executing plans to maximize the expected value of this function.', 'They are closely related to agents in economics and are studied in various fields including cognitive science, ethics, and socio-cognitive modeling.', \"Intelligent agents are often described as abstract functional systems, with terms like 'abstract intelligent agents' distinguishing theoretical models from real implementations.\", \"They are also related to software agents, which are autonomous programs performing tasks for users, and are sometimes referred to as 'rational agents' from economics.\"]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a structured output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=[\n",
    "    ResponseSchema(name=\"first_fact\", description=\"first fact about text\"),\n",
    "    ResponseSchema(name=\"second_fact\", description=\"second fact about text\"),\n",
    "    ResponseSchema(name=\"third_fact\", description=\"third fact about text\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StructuredOutputParser.from_response_schemas(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template3 = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = template3 | groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result=chain.invoke({\"topic\":topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_fact': 'Intelligent agents can range from simple to highly complex, including examples such as a basic thermostat, a human being, or even large systems like firms, states, or biomes.', 'second_fact': 'Intelligent agents operate based on an objective function that encapsulates their goals, designing plans to maximize the expected value of this function. For instance, reinforcement learning agents use a reward function, while evolutionary algorithms use a fitness function.', 'third_fact': \"Intelligent agents are closely related to concepts in economics, cognitive science, ethics, and philosophy. They can be described as abstract functional systems or software agents, and are sometimes referred to as 'rational agents' in interdisciplinary contexts.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydantic Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name:str=Field(description=\"name of person\")\n",
    "    age:int=Field(gt=18,description=\"age of person\")\n",
    "    city:str=Field(description=\"name of the city where the person is located\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=PydanticOutputParser(pydantic_object=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"name of person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"age of person\", \"exclusiveMinimum\": 18, \"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"name of the city where the person is located\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"city\"]}\\n```'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = template | groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke({\"place\":\"bengaluru\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Rohan Sharma' age=27 city='Bengaluru'\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Original Text\n",
    "text = \"\"\"LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\n",
    "\n",
    "LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.\n",
    "\n",
    "The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\n",
    "\n",
    "With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\n",
    "\n",
    "The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\n",
    ".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the splitter\n",
    "splitter = CharacterTextSplitter( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,    \n",
    "    chunk_overlap=50 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 518, which is longer than the specified 200\n",
      "Created a chunk of size 355, which is longer than the specified 200\n",
      "Created a chunk of size 346, which is longer than the specified 200\n",
      "Created a chunk of size 361, which is longer than the specified 200\n",
      "Created a chunk of size 387, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Splitting the text\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.',\n",
       " 'LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.',\n",
       " \"The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\",\n",
       " 'With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.',\n",
       " 'The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.',\n",
       " '.']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\n",
      "--------------------------------------------------\n",
      "Chunk 4:\n",
      "With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\n",
      "--------------------------------------------------\n",
      "Chunk 5:\n",
      "The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\n",
      "--------------------------------------------------\n",
      "Chunk 6:\n",
      ".\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Printing the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\\n\\nLangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.',\n",
       " \"The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\\n\\nWith built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\",\n",
       " 'The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\\n.']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI's encoding\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\n",
      "\n",
      "LangChain provides seamless support\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      " This makes LangChain a great choice for developers working on AI-powered applications.\n",
      "\n",
      "LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.\n",
      "\n",
      "The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "LM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\n",
      "\n",
      "With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain\n",
      "--------------------------------------------------\n",
      "Chunk 4:\n",
      " over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\n",
      "\n",
      "The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust\n",
      "--------------------------------------------------\n",
      "Chunk 5:\n",
      " AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\n",
      ".\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Printing the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
